\chapter{Models and Methods}

The models presented in chapter~\ref{relevant-models} seem to work well either in the context of adaptive systems where we aren't concerned with timing information or have been tested in controlled environment where the students had no prior knowledge of the presented material. In this chapter, we elaborate the methods we used to further improve the performance by taking into account the timing information of students' answers, i.e. the response times or the breaks between presentations (the timing distance). We are interested primarily in domains where the prior knowledge varies widely between students.

\section{Models Based on Timing Information}

In this section we discuss several models which aim at modeling students' memory with the usage of timing information of answers. There are several issues to consider beforehand:

\begin{itemize}
  \item Students have different skills and thus the rate of retention loss varies.
  \item The difficulty of items varies, some items are forgotten faster and some slower.
  \item The prior knowledge of each student is different. If the student already learned the practiced item and has it stored in their long term memory, the process of forgetting is much slower.
\end{itemize}

\subsection{The Extended Model}
\label{pfaet}

The PFA Extended model summarized in chapter~\ref{pfae} can be further enhanced when we utilize the timing information by locally changing the memory activation in prediction. The difference in seconds between the times of the current question and the last answer is passed to a \textit{time effect function} (we adapted this term from a related work~\cite{Pelanek2015}). The time effect function increases or decreases the probability of recall or the memory activation depending on the timing distance. The updated equation with a time effect function is depicted in Equation~\ref{eq-pfa-standard-time-p}.

\begin{equation} \label{eq-pfa-standard-time-p}
  P(m) = \frac{1}{1 + e^{-(m + f(t))}}
\end{equation}

The advantage of this model is that it shares the same properties as the presented PFA Extended model, i.e. it's straightforward to employ Elo model for estimation of prior knowledge as well as to account for the probability of guessing.

\subsection{The Alternative Model}
\label{pfagt}

Another way of dealing with timing distances between attempts is by changing the decay factor $\xi$ of the model presented in chapter~\ref{pfag}. The model takes into account the order of questions, yet doesn't consider brakes between student's practices of an item. This problem can be resolved by replacing the parameter $\xi$ with a time effect function.

\begin{equation} \label{eq-pfa-gong-time-s}
  s_{i,j} = \sum_{k=1}^{n-1} y_k \cdot f(t_k)
\end{equation}

\begin{equation} \label{eq-pfa-gong-time-f}
  f_{i,j} = \sum_{k=1}^{n-1} |y_k - 1| \cdot g(t_k)
\end{equation}

Equations~\ref{eq-pfa-gong-time-s} and~\ref{eq-pfa-gong-time-f} show the incorporation of the time effect function $f$ and $g$ in the model. The parameter $t_k$ represents the number of seconds that passed between the $k$-th practice and the most recent one. The weight of successes and failures is thus dependent on the ages of the prior practices.

As was discussed in the chapter~\ref{pfa}, the problem arises with multiple-choice questions. Another complication is the choice of some good time effect functions that fit the data well. Note that in this model, we often set $\gamma = \delta = 1$, the weight of each success and failure is then entirely dependent on the parameters of the chosen time effect functions instead of $\gamma$ and $\delta$.

\subsection{The Response Time}
\label{pfart}

The response time of student may indicate student's level of learning. If the student answers quickly, it often implies the level of automaticity. We address this phenomenon by increasing the memory activation $m$ after each answered item.

\begin{equation} \label{eq-pfa-extended-rt}
  m \gets \begin{cases}
            m + \gamma \cdot (1 - P(m)) + r(t_d), & \text{\textbf{if }} \text{the answer was correct} \\
            m + \delta \cdot P(m) + r(t_d), & \text{\textbf{otherwise}}
          \end{cases}
\end{equation}

Equation~\ref{eq-pfa-extended-rt} shows the adjusted update rule with a unary function $r(t_d)$, the parameter $t_d$ is the difference between the time the item was presented and the time it was answered (i.e. it's the student's delay).

\section{Quantifying the Quality of Predictions}

One way to quantify the quality of model predictions is to use metric functions. A good choice of a metric function is important for accurate evaluation of performance of student models. In our case we aren't interested strictly in the correctness of student's answers, the goal is to precisely assess their knowledge. The information of student's knowledge is crucial if we don't want to demotivate the student by too difficult or simple questions. For this purpose a good choice is the Root Mean Squared Error metric (see~Equation~\ref{rmse}) or the less popular Log Likelihood (see~Equation~\ref{ll})~\cite{Pelanek2015a}.

\begin{equation} \label{rmse}
  RMSE = \sqrt{\frac{\sum_{i=1}^n (p_i - y_i)^2}{n}}
\end{equation}

\begin{equation} \label{ll}
  LL = \sum_{i=1}^n y_i \log(p_i) + (1 - y_i) \log(1 - p_i)
\end{equation}

The RMSE metric represents ``error'', its the square root of the sum of all squared differences between predicted values $p_i$ and true values $y_i$ divided by the number of samples in data set. RMSE yields a number between 0 and 1, where 0 represents perfect predictions. LL metric is the logarithm of likelihood, the LL value is a negative number dependent on the size of the data set (the bigger the dataset, the lower LL value is possible). Higher values of LL represent the better predictions.

In domains such as information retrieval or pattern recognition other types of metrics are commonly used which are based on qualitative understanding of errors~\cite{Pelanek2015a} instead of their probabilistic value. An advantage of this type of metrics is that it can be easily used in multi-classification tasks. These metrics, however, depend on a chosen threshold, e.g. in case the threshold is set to $0.5$ the predictions $0.51$ and $0.98$ are classified as positive while predictions $0.49$ and $0.04$ are classified as negative.

\begin{table}[htbp]
  \centering
  \caption{Confusion matrix.}
  \begin{tabular}{ l l c c }
   \toprule[\heavyrulewidth]
   & & \multicolumn{2}{c}{\textbf{Predicted Outcome}} \\
   & & Positive & Negative \\
   \midrule[\heavyrulewidth]
   \multirow{2}{5em}{\textbf{Observed Value}}
   & Positive & True Positive (TP) & False Negative (FN) \\
   & Negative & False Positive (FP) & True Negative (TN) \\
   \bottomrule[\heavyrulewidth]
  \end{tabular}
  \label{table:confusion-matrix}
\end{table}

The Table~\ref{table:confusion-matrix} shows the confusion matrix of binary classifier (i.e. the mislabeling of classes). Once the confusion of classes is calculated, we can derivate for example precision (see~Equation~\ref{precision}) and accuracy (see~Equation~\ref{accuracy}).

\begin{equation} \label{precision}
  \mathit{Precision} = \frac{TP}{TP + FP}
\end{equation}

\begin{equation} \label{accuracy}
  \mathit{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}
\end{equation}

Even though the precision and accuracy aren't very fit in our case as we are more interested in probabilistic understanding of errors, another commonly used metric based on marking predictions as true/false positives/negatives is Area Under the ROC Curve (AUC). In this metric, we measure the performance across all possible thresholds. The result is a number between $0.5$ and $1$, where $1$ represents perfect predictions of the classifier. Note that if all predictions are divided by 2, AUC stays the same, this property is sometimes criticized---hence, it should be used with caution~\cite{Pelanek2015a}.

\section{Parameter Estimation}

The goal of the fitting procedure is to find the optimal parameters that perform well on new samples (in our case predict the student's ability to answer a question correctly). In this section we describe some metrics commonly used for the evaluation of predictive performance (i.e. the fitness of parameters in the model) and the principal algorithms that are suitable for parameter estimation in our case.

\subsection{Grid Search}

This technique is used for parameter optimization, it exhaustively generates a grid of all combinations of selected parameter values where each point in the grid is a function value of a chosen metric (e.g. RMSE). The fittest combination of parameters is represented by the best score of the metric (e.g. has lowest error). The figure~\ref{fig-grid-search-rmse-auc} portrays an example of the grid search on PFA model with two parameters $\gamma$ and $\delta$.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{img/pfa-grid-search-rmse-auc}
  \caption{Result of the grid search performed on the PFAE model.}
  \label{fig-grid-search-rmse-auc}
\end{figure}

The disadvantage of this optimization method is the high computational complexity especially in cases when the model contains a lot of parameters. Also, there is no simple way to plot or visualize a high-dimensional grid.

\subsection{Hill Climbing}
\label{hill-climbing}

This method starts from a given position in the parameter space and evaluates the score of an objective function (metric function in our case). The model is then trained using the values from the enclosing neighborhood of the parameter space, afterwards the best combination of parameters is selected and the process repeated. This continues until there is no other parameter value in the neighborhood with a better score of the objective function, i.e. we found the local minimum~\cite{Russell2009}. TODO: elaborate, maybe write pseudocode, or at least more formal description.

\subsection{Gradient Descent}
\label{gradient-descent}

Gradient descent is optimization algorithm very similar to hill climbing, the difference is that gradient descent has indication which way to ``climb''. The goal of gradient descent is to find the best parameters for a function called hypothesis~\cite{Klusasek2014}, this is done by finding the lowest value of the cost function $J(\rho)$ (similar to objective function in hill climbing). The cost function represents the error of a chosen hypothesis $h_{\rho}(x_i) = \rho^T x_i = \sum^n_{j=0} \rho_j x_{i,j}$, where $\rho_j$ is the value of the $j$-th parameter and $x_{i,j}$ the $i$-th value of the $j$-th feature (e.g. the skill of the student $i$).

\begin{equation} \label{cost-function}
  J(\rho) = \frac{1}{2m} \sum^m_{i=1} (h_{\rho}(x_i) - y_i)^2
\end{equation}

Formal definition of the cost function is depicted in Equation~\ref{cost-function}, it is the sum of all squared errors of hypotheses $h_{\rho}$ over all examples from data set of size $m$ divided by $2m$. Our is to minimize the value of $J(\rho)$ which can be done efficiently by the estimation of the gradient using the following update rule:

\begin{equation} \label{cost-function-update}
  \rho_j \gets \rho_j - \alpha \frac{\partial}{\partial \rho_j} J(\rho)~~\text{for all } j
\end{equation}

The partial derivatives help indicate the surface of the cost function, it gives us the information which direction to take in order to reach the closest local minimum. The value of $\alpha$ is the size of one step or a learning rate, if the value is too big the algorithm might not be able to ever reach a local minimum, in contrast if the value is very small, it is less efficient and the computation takes longer. Now we can imagine that the main difference between gradient descent and hill climbing is that the latter doesn't have any indication of which turn to take next and how big one step in a direction should be~\cite{Russell2009}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{img/gradient-descent}
  \caption{3D visualization showing first 7 iterations of gradient descent applied to an example with two parameters.}
  \label{fig-gradient-descent}
\end{figure}

\subsection*{Gradient Descent in Online Learning}

In online learning, straightforward way to use optimization methods such as gradient descent doesn't exist. We can, however reuse some aspects of several algorithms used in machine learning and data mining for parameter optimization.

The most obvious and easiest way to implement gradient descent in cases where there is no derivative of the object function could average the difference between observations (correctness of student's answer $y_i$) and predictions (model's predicted probability that the user $i$ answers correctly $p_i$). The batch update rule could be defined as follows:

\begin{equation} \label{online-learning-batch-rule}
 \rho_j \gets \rho_j - \alpha~\frac{1}{n}\sum_{i=0}^n (p_i - y_i)~~\text{for all } j
\end{equation}

It is easy to demonstrate that this method doesn't work well even when the number of parameters is very low. The parameters get easily stuck in the local minimum as can be observed in the Figure~\ref{fig-grid-search-rmse-off}. Another disadvantage of this technique is that we need to relearn the optimal parameters by repeating the process of evaluation periodically on the whole data set.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{img/pfa-grid-search-rmse-off}
  \caption{Result of the grid search performed on the PFAE model with 2 parameters. The figure on the right hand size shows the averages of $p_i - y_i$ over all answers for a grid of parameters.}
  \label{fig-grid-search-rmse-off}
\end{figure}

Radek Pel√°nek in his work~\cite{Pelanek2015} figured out a better, more complicated way of parameter estimation which combines some aspects of gradient descent and is suitable in online learning. The update of parameters happens after arrival of every new data point (i.e. student's answer). This method is very practical particularly in our case where we need to find the most optimal and best calibrated time effect function. TODO: elaborate, properly describe the method and its applications, pseudocode

\section{Time Effect Functions}
\label{time-effect-functions}

In chapter~\ref{memory} we mentioned that forgetting usually respects the power law, which is often true in cases where the students have no prior knowledge of the practiced material. For our purpose, we have chosen the following functions with parameter $t$ representing the time of the last presentation and two parameters $a$ and $c$:

\begin{itemize}
  \item $f_{\mathit{log}}(t) = a - c \cdot \log(t)$
  \item $f_{\mathit{exp}}(t) = a \cdot e^{-c \sqrt{t}}$
  \item $f_{\mathit{poly}}(t) = a / t^c$
\end{itemize}

The shape of each of these time effect functions with logarithmically scaled $x$-axis is depicted on Figure~\ref{fig-time-effect-functions}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{img/time-effect-functions}
  \caption{Candidates of time effect functions we inspected and evaluated in our analysis. Note that the $x$-axis is log scaled.}
  \label{fig-time-effect-functions}
\end{figure}

\subsection{The Staircase Function}
\label{staircase-function}

If we want to approximate the exact shape of a time effect function, we can define a staircase function with fixed intervals $\iota$. In each interval $(\iota_i, \iota_{i+1}]$, we preserve a learned value $a_i$ which represents an increase in memory activation. The formal definition of the staircase function $f_{\mathit{staircase}}$ is formalized in Equation~\ref{eq-staircase}.

\begin{equation} \label{eq-staircase}
  f_{\mathit{staircase}}(t) = \begin{cases}
            a_1, & \text{if } \iota_0 < t \leq \iota_1 \\
            a_2, & \text{if } \iota_1 < t \leq \iota_2 \\
                 & \hspace{1em} \dots \\
            a_n, & \text{if } \iota_{n-1} < t \leq \iota_n
         \end{cases}
\end{equation}

Applying simple linear algebra, we can further modify the staircase function so that the memory activation between two points is a linear function, this makes a better approximation of the learned values. The definition of the adjusted function is described in Equations~\ref{eq-staircase-iota-prime-definiton} and~\ref{eq-staircase-linear}, where $T$ is a set of all timing distances of all answers.

\begin{equation} \label{eq-staircase-iota-prime-definiton}
\begin{split}
  \hat{\iota}_0 &= \iota_0 \\
  \hat{\iota}_i &= \mathit{mean}\{t \in T~|~\iota_{i-1} < t \leq \iota_i\}
\end{split}
\end{equation}

\begin{equation} \label{eq-staircase-linear}
  f_{\mathit{staircase}}(t) = \begin{cases}
            a_1, & \text{if } \hat{\iota}_0 < t \leq \hat{\iota}_1 \\
            (t - \hat{\iota}_1) \frac{a_2 - a_1}{\hat{\iota}_2 - \hat{\iota}_1} + a_1, & \text{if } \hat{\iota}_1 < t \leq \hat{\iota}_2 \\
            \hspace{9em} \dots \\
            (t - \hat{\iota}_{n-1}) \frac{a_n - a_{n-1}}{\hat{\iota}_n - \hat{\iota}_{n-1}} + a_{n-1}, & \text{if } \hat{\iota}_{n-1} < t \leq \hat{\iota}_n     
         \end{cases}
\end{equation}

Note that $\hat{\iota}_0$ is in our case always equal to $0$ and $\hat{\iota}_n$ to infinity (in which case the memory activation in the interval is equal to $a_n$).
